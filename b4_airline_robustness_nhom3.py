# -*- coding: utf-8 -*-
"""B4. Airline-Robustness-Nhom3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14SQtRlud-1hdZzfAw_l9ubED7DNQylQe

# Prepare Data
"""

from pathlib import Path

# === Paths ===
DATA_DIR = Path("./data")
OUT_DIR  = Path("./outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)

AIRPORTS   = DATA_DIR / "airports.dat"
AIRLINES   = DATA_DIR / "airlines.dat"
ROUTES     = DATA_DIR / "routes.dat"
PLANES     = DATA_DIR / "planes.dat"
COUNTRIES  = DATA_DIR / "countries.dat"

print("Files present:",
      AIRPORTS.exists(), AIRLINES.exists(), ROUTES.exists(), PLANES.exists(), COUNTRIES.exists())

import numpy as np, pandas as pd, networkx as nx, matplotlib.pyplot as plt

plt.rcParams.update({"figure.dpi": 130})

AIRPORTS_COLS = ["airport_id","name","city","country","iata","icao","lat","lon","altitude","timezone","dst","tz_db","type","source"]
AIRLINES_COLS = ["airline_id","name","alias","iata","icao","callsign","country","active"]
ROUTES_COLS   = ["airline_code","airline_id","src_code","src_id","dst_code","dst_id","codeshare","stops","equipment"]
PLANES_COLS   = ["name","iata_code","icao_code"]
COUNTRIES_COLS= ["name","iso_code","dafif_code"]

def load_all(airports, airlines, routes, planes, countries):
    ap = pd.read_csv(airports, header=None, names=AIRPORTS_COLS, na_values=["\\N"])
    al = pd.read_csv(airlines, header=None, names=AIRLINES_COLS, na_values=["\\N"])
    rt = pd.read_csv(routes,   header=None, names=ROUTES_COLS,   na_values=["\\N"])
    pl = pd.read_csv(planes,   header=None, names=PLANES_COLS,   na_values=["\\N"])
    co = pd.read_csv(countries, header=None, names=COUNTRIES_COLS, na_values=["\\N"])


    ap = ap.dropna(subset=["airport_id"])
    ap["airport_id"] = ap["airport_id"].astype(int)
    ap["iata"] = ap["iata"].astype(str).str.strip().str.upper()
    ap["icao"] = ap["icao"].astype(str).str.strip().str.upper()
    ap["country"] = ap["country"].astype(str).str.strip()

    # airlines
    al["airline_id"] = pd.to_numeric(al["airline_id"], errors="coerce")
    al = al.dropna(subset=["airline_id"])
    al["airline_id"] = al["airline_id"].astype(int)
    al["iata"] = al["iata"].astype(str).str.strip().str.upper()
    al["icao"] = al["icao"].astype(str).str.strip().str.upper()

    # routes
    rt["src_id"] = pd.to_numeric(rt["src_id"], errors="coerce")
    rt["dst_id"] = pd.to_numeric(rt["dst_id"], errors="coerce")
    rt = rt.dropna(subset=["src_id","dst_id"])
    rt["src_id"] = rt["src_id"].astype(int)
    rt["dst_id"] = rt["dst_id"].astype(int)
    rt["equipment"] = rt["equipment"].fillna("").astype(str).str.strip()

    # planes
    pl["iata_code"] = pl["iata_code"].astype(str).str.strip().str.upper()
    pl["icao_code"] = pl["icao_code"].astype(str).str.strip().str.upper()
    pl["name"] = pl["name"].astype(str).str.strip()

    # countries
    co["name"] = co["name"].astype(str).str.strip()

    return ap, al, rt, pl, co

ap, al, rt, pl, co = load_all(AIRPORTS, AIRLINES, ROUTES, PLANES, COUNTRIES)
ap.shape, al.shape, rt.shape, pl.shape, co.shape

print("Airports NA summary:\n", ap.isna().sum(), "\n")
print("Airlines NA summary:\n", al.isna().sum(), "\n")
print("Routes NA summary:\n", rt.isna().sum(), "\n")

print("Sample airports:\n", ap.head(3), "\n")
print("Sample airlines:\n", al.head(3), "\n")
print("Sample routes:\n", rt.head(3))

"""# Build Graph"""

def build_undirected_graph(ap, rt):
    valid = set(ap["airport_id"])
    r = rt[(rt["src_id"].isin(valid)) & (rt["dst_id"].isin(valid))]
    r = r[r["src_id"] != r["dst_id"]].copy()

    G = nx.Graph()
    G.add_nodes_from(ap["airport_id"].tolist())
    G.add_edges_from(zip(r["src_id"], r["dst_id"]))

    attrs = ap.set_index("airport_id")[["name","city","country","iata","icao","lat","lon"]].to_dict("index")
    nx.set_node_attributes(G, attrs)
    return G, r

G, r_clean = build_undirected_graph(ap, rt)
G.number_of_nodes(), G.number_of_edges()

print(f"Số node ban đầu: {G.number_of_nodes()}")
print(f"Số cạnh ban đầu: {G.number_of_edges()}")

# 1. Tìm thành phần liên thông lớn nhất (LCC)
largest_cc_nodes = max(nx.connected_components(G), key=len)

# 2. Tạo đồ thị con chỉ chứa các node này
G_clean = G.subgraph(largest_cc_nodes).copy()

# 3. Cập nhật lại đồ thị G để dùng cho các bước sau
G = G_clean
N0 = G.number_of_nodes() # Cập nhật N0 mới

print(f"Số node sau khi lọc (LCC): {G.number_of_nodes()}")
print(f"Số cạnh sau khi lọc: {G.number_of_edges()}")

"""# Metrics"""

!pip install networkit

from networkx.algorithms import approximation as apx
import networkit as nk
from math import inf
import random

def lcc_nodes(G):
    return max(nx.connected_components(G), key=len) if G.number_of_nodes() > 0 else set()

def lcc_fraction(G, N0):
    return (len(lcc_nodes(G))/float(N0)) if G.number_of_nodes() > 0 and N0>0 else 0.0

def approx_diameter_of_LCC(G):
    if G.number_of_nodes() == 0:
        return float("nan")
    L = G.subgraph(lcc_nodes(G)).copy()
    if L.number_of_nodes() <= 1:
        return 0.0
    try:
        return float(apx.diameter(L))  # 2-sweep lower bound
    except Exception:
        # fallback: 90th percentile of sampled shortest paths
        nodes = list(L.nodes())[:min(100, L.number_of_nodes())]
        lens = []
        for s in nodes:
            lens.extend(nx.single_source_shortest_path_length(L, s).values())
        return float(np.percentile(lens, 90)) if len(lens)>0 else float("nan")

def approx_avg_path_of_LCC(G, max_samples=500):
    if G.number_of_nodes() == 0:
        return float("nan")
    L = G.subgraph(lcc_nodes(G)).copy()
    n = L.number_of_nodes()
    if n <= 1:
        return 0.0
    node_list = list(L.nodes())
    id_map = {node: i for i, node in enumerate(node_list)}
    # 3. Tạo graph NetworKit
    G_nk = nk.graph.Graph(n, weighted=False, directed=False)
    for u, v in L.edges():
        G_nk.addEdge(id_map[u], id_map[v])

    # 4. Tính all-pairs shortest paths với NetworKit
    apsp = nk.distance.APSP(G_nk)
    apsp.run()

    # 5. Lấy tất cả khoảng cách hữu hạn và tính trung bình
    dists = []
    for i in range(n):
        for j in range(i+1, n):
            d = apsp.getDistance(i, j)
            if d < inf:
                dists.append(d)

    return float(np.mean(dists)) if dists else float("nan")

def R_index(fracs, curve):
    return float(np.trapezoid(curve, fracs))

"""## Configuration Experiment"""

# Configure experiment
FRACS = np.linspace(0, 1, 51) # 0%..100%
TRIALS = 20
ATTACK_MODE = "static"

"""# Attack"""

def compute_centrality(G, method="degree", k_approx=None):
    """
    Computes centrality for all nodes in G.

    Args:
        G: NetworkX graph
        method: 'degree', 'pagerank', or 'betweenness'
        k_approx: int (optional), number of sample nodes for approx betweenness
                  (use ~100-200 for large graphs to speed up).
    Returns:
        Dictionary {node: score}
    """
    if method == "degree":
        return dict(G.degree())

    elif method == "pagerank":
        # PageRank hoạt động tốt trên cả undirected (tương tự eigenvector centrality)
        try:
            return nx.pagerank(G, alpha=0.85)
        except nx.PowerIterationFailedConvergence:
            # Fallback nếu không hội tụ (hiếm gặp với undirected)
            return dict(G.degree())

    elif method == "betweenness":
        # Nếu k_approx được set, dùng xấp xỉ để chạy nhanh hơn
        if k_approx and k_approx < len(G):
            return nx.betweenness_centrality(G, k=k_approx, normalized=True)
        else:
            return nx.betweenness_centrality(G, normalized=True)

    else:
        raise ValueError(f"Unknown centrality method: {method}")

def random_removal_curves(G, fracs, trials=10, seed=123):
    rng = random.Random(seed)
    N0 = G.number_of_nodes()
    nodes_all = list(G.nodes())
    lcc_vals, dia_vals, apl_vals  = [], [], []
    for f in fracs:
        k = int(round(f*N0))
        lcc_trial, dia_trial, apl_trial  = [], [], []
        for _ in range(trials):
            nodes = nodes_all[:]
            rng.shuffle(nodes)
            H = G.copy()
            H.remove_nodes_from(nodes[:k])
            lcc_trial.append(lcc_fraction(H, N0))
            dia_trial.append(approx_diameter_of_LCC(H))
            apl_trial.append(approx_avg_path_of_LCC(H))

        lcc_vals.append(float(np.mean(lcc_trial)))
        dia_vals.append(float(np.nanmean(dia_trial)))
        apl_vals.append(float(np.nanmean(apl_trial)))
    return np.array(lcc_vals), np.array(dia_vals), np.array(apl_vals)

def targeted_degree_curves(G, fracs, method="degree", mode="static", k_approx=None):
    N0 = G.number_of_nodes()
    lcc_vals, dia_vals, apl_vals  = [], [], []
    if mode == "static":
        cent = compute_centrality(G, method=method, k_approx=k_approx)
        order = [n for n,_ in sorted(cent.items(), key=lambda x:(-x[1], x[0]))]
        for f in fracs:
            k = int(round(f*N0))
            H = G.copy()
            H.remove_nodes_from(order[:k])
            lcc_vals.append(lcc_fraction(H, N0))
            dia_vals.append(approx_diameter_of_LCC(H))
            apl_vals.append(approx_avg_path_of_LCC(H))
    else:
        print(f"--- Running ADAPTIVE {method} attack ---")
        H = G.copy()

        # Chuyển fracs thành số lượng node cần xóa tương ứng
        # Ví dụ: fracs=[0.1, 0.2] -> targets=[100, 200]
        targets = [int(round(f * N0)) for f in fracs]

        # Cần đảm bảo targets tăng dần để simulation chạy tuần tự
        # (Giả sử input fracs đã sort, nếu chưa thì cần sort và map lại kết quả)

        removed_count = 0
        current_target_idx = 0

        # Lưu kết quả cho f=0 (nếu có trong fracs)
        if 0.0 in fracs:
             lcc_vals.append(lcc_fraction(H, N0))
             dia_vals.append(approx_diameter_of_LCC(H))
             apl_vals.append(approx_avg_path_of_LCC(H))
             current_target_idx += 1

        while current_target_idx < len(targets) and H.number_of_nodes() > 0:
            target_k = targets[current_target_idx]

            # Xóa dần cho đến khi đạt target tiếp theo
            while removed_count < target_k and H.number_of_nodes() > 0:
                # Tính lại centrality trên đồ thị hiện tại H
                cent = compute_centrality(H, method=method, k_approx=k_approx)

                # Tìm node max score
                best_node = max(cent.items(), key=lambda x: x[1])[0]

                H.remove_node(best_node)
                removed_count += 1

            # Đã đạt mốc f, ghi lại kết quả
            lcc_vals.append(lcc_fraction(H, N0))
            dia_vals.append(approx_diameter_of_LCC(H))
            apl_vals.append(approx_avg_path_of_LCC(H))

            current_target_idx += 1

    return np.array(lcc_vals), np.array(dia_vals), np.array(apl_vals)

# Random failures
rand_LCC, rand_DIA, rand_APL = random_removal_curves(G, FRACS, trials=TRIALS, seed=123)

k_approx_value = min(200, G.number_of_nodes() // 2)
# Targeted attacks by degree
targ_LCC_degree, targ_DIA_degree, targ_APL_degree = targeted_degree_curves(G, FRACS, method="degree", mode=ATTACK_MODE)
targ_LCC_pagerank, targ_DIA_pagerank, targ_APL_pagerank = targeted_degree_curves(G, FRACS, method="pagerank", mode=ATTACK_MODE)
targ_LCC_betweenness, targ_DIA_betweenness, targ_APL_betweenness = targeted_degree_curves(G, FRACS, method="betweenness", mode=ATTACK_MODE,k_approx=k_approx_value)

R_rand = R_index(FRACS, rand_LCC)
R_targ_degree = R_index(FRACS, targ_LCC_degree)
R_targ_pagerank = R_index(FRACS, targ_LCC_pagerank)
R_targ_betweenness = R_index(FRACS, targ_LCC_betweenness)

print(f"R-index(LCC) — Random: {R_rand:.3f}")
print(f"R-index(LCC) — Targeted (Degree): {R_targ_degree:.3f}")
print(f"R-index(LCC) — Targeted (PageRank): {R_targ_pagerank:.3f}")
print(f"R-index(LCC) — Targeted (Betweenness): {R_targ_betweenness:.3f}")

"""### Consolidated Plotting
To streamline the plotting, a new function `plot_robustness_curves` is introduced. This function takes the data for random and targeted attacks across different centrality measures (degree, PageRank, betweenness) and generates a single plot. This approach promotes code reusability and makes it easier to compare the robustness metrics.
"""

def plot_robustness_curves(title, ylabel, rand_curve, targ_degree_curve, targ_pagerank_curve, targ_betweenness_curve, filename):
    plt.figure(figsize=(8,5))
    plt.plot(FRACS*100, rand_curve, label=f"Random (trials={TRIALS})")
    plt.plot(FRACS*100, targ_degree_curve, label=f"Targeted-degree ({ATTACK_MODE})")
    plt.plot(FRACS*100, targ_pagerank_curve, label=f"Targeted-PageRank ({ATTACK_MODE})")
    plt.plot(FRACS*100, targ_betweenness_curve, label=f"Targeted-Betweenness ({ATTACK_MODE})")
    plt.xlabel("Fraction of airports removed (%)")
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend(); plt.tight_layout()
    plt.savefig(OUT_DIR/filename)
    print(OUT_DIR/filename)

# Plot LCC
plot_robustness_curves(
    "OpenFlights Robustness — LCC",
    "LCC size / N0",
    rand_LCC, targ_LCC_degree, targ_LCC_pagerank, targ_LCC_betweenness,
    "robustness_lcc.png"
)

# Plot Diameter
plot_robustness_curves(
    "OpenFlights Robustness — Diameter (approx.)",
    "Approx. Diameter of LCC (hops)",
    rand_DIA, targ_DIA_degree, targ_DIA_pagerank, targ_DIA_betweenness,
    "robustness_diameter.png"
)

# Plot Average Path Length
plot_robustness_curves(
    "OpenFlights Robustness — Average Path Length (approx.)",
    "Approx. Average Path Length",
    rand_APL, targ_APL_degree, targ_APL_pagerank, targ_APL_betweenness,
    "robustness_Average_Path_Length.png"
)

"""# Defense

## TITS2018
"""

!pip install scipy geopy

import networkx as nx
import numpy as np
import random
from geopy.distance import great_circle
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import splu
from typing import Any, List, Tuple, Dict, Optional


# ----------------------------
# 1) Build (grounded) Laplacian factorization
# ----------------------------
def _build_grounded_laplacian_lu(G: nx.Graph, node_list: List[Any]) -> Tuple[Dict[Any,int], int, Any, Any]:
    """
    Returns:
      idx: node->0..n-1
      g: ground index
      ground_node
      LU factorization of grounded Laplacian (n-1 x n-1)
    """
    n = len(node_list)
    idx = {u:i for i,u in enumerate(node_list)}
    g = n - 1                      # choose last node as ground
    ground_node = node_list[g]

    # adjacency (unweighted conductance=1). For weighted, use weight attr as conductance.
    rows, cols, data = [], [], []
    deg = np.zeros(n, dtype=float)

    for u, v in G.edges():
        iu, iv = idx[u], idx[v]
        w = 1.0
        if iu == iv:
            continue
        # off-diagonals -w
        rows += [iu, iv]
        cols += [iv, iu]
        data += [-w, -w]
        # diagonals accumulate degree
        deg[iu] += w
        deg[iv] += w

    # add diagonals
    rows += list(range(n))
    cols += list(range(n))
    data += deg.tolist()

    L = csr_matrix((data, (rows, cols)), shape=(n, n))

    # grounded Laplacian: remove row/col g -> SPD if connected
    mask = np.ones(n, dtype=bool)
    mask[g] = False
    Lg = L[mask][:, mask].tocsc()

    lu = splu(Lg)  # sparse LU; works well for n~few thousands
    return idx, g, ground_node, lu


def effective_resistance(lu, idx: Dict[Any,int], g: int, u: Any, v: Any) -> float:
    """
    Exact effective resistance for unweighted graph using one Laplacian solve:
      Solve L x = e_u - e_v with x_ground=0 (grounding removes singularity).
      R_eff(u,v) = (e_u - e_v)^T x = x[u] - x[v]
    """
    n = len(idx)
    iu, iv = idx[u], idx[v]

    # build b (n-1)
    b = np.zeros(n-1, dtype=float)

    def red(i):
        # map full index -> reduced index (skip ground g)
        return i if i < g else i - 1

    if iu != g:
        b[red(iu)] += 1.0
    if iv != g:
        b[red(iv)] -= 1.0

    x = lu.solve(b)  # reduced solution

    def x_full(i):
        if i == g:
            return 0.0
        return x[red(i)]

    return float(x_full(iu) - x_full(iv))


# ----------------------------
# 2) Candidate generation (random sampling + distance filter)
# ----------------------------
def geo_dist_km(G: nx.Graph, u: Any, v: Any) -> Optional[float]:
    nu, nv = G.nodes[u], G.nodes[v]
    if ("lat" in nu and "lon" in nu and "lat" in nv and "lon" in nv):
        try:
            return float(great_circle((nu["lat"], nu["lon"]), (nv["lat"], nv["lon"])).kilometers)
        except Exception:
            return None
    return None


def sample_candidate_edges(
    G: nx.Graph,
    max_candidates: int = 20000,
    max_distance_km: Optional[float] = 3000.0,
    seed: int = 0
) -> List[Tuple[Any, Any, Optional[float]]]:
    """
    Randomly sample non-edges as candidates, filtered by distance if coords exist.
    """
    rng = random.Random(seed)
    nodes = list(G.nodes())
    n = len(nodes)
    existing = set((u, v) if u <= v else (v, u) for u, v in G.edges())

    candidates = []
    tries = 0
    max_tries = max_candidates * 50  # avoid infinite loop

    while len(candidates) < max_candidates and tries < max_tries:
        tries += 1
        u = nodes[rng.randrange(n)]
        v = nodes[rng.randrange(n)]
        if u == v:
            continue
        a, b = (u, v) if u <= v else (v, u)
        if (a, b) in existing:
            continue

        d = geo_dist_km(G, u, v)
        if max_distance_km is not None and d is not None and d > max_distance_km:
            continue

        candidates.append((u, v, d))
        existing.add((a, b))  # prevent duplicates in candidate list too

    return candidates


# ----------------------------
# 3) Select k edges by highest effective resistance (TER-inspired heuristic)
# ----------------------------
def add_edges_by_effective_resistance(
    G: nx.Graph,
    k: int = 200,
    max_candidates: int = 20000,
    max_distance_km: Optional[float] = 3000.0,
    seed: int = 0
) -> Tuple[nx.Graph, List[Tuple[Any, Any, Dict[str, Any]]]]:
    """
    Build candidate edges, compute R_eff(u,v), add top-k.
    """
    if G.number_of_nodes() < 2:
        return G.copy(), []

    # work on LCC only (recommended)
    lcc = max(nx.connected_components(G), key=len)
    H = G.subgraph(lcc).copy()

    node_list = list(H.nodes())
    idx, g, ground_node, lu = _build_grounded_laplacian_lu(H, node_list)

    cand = sample_candidate_edges(H, max_candidates=max_candidates,
                                  max_distance_km=max_distance_km, seed=seed)

    scored = []
    for u, v, d in cand:
        reff = effective_resistance(lu, idx, g, u, v)
        scored.append((reff, u, v, d))

    scored.sort(reverse=True, key=lambda x: x[0])

    Hr = H.copy()
    added = []
    for reff, u, v, d in scored[:k]:
        meta = {"defense": "TER_like", "Reff": float(reff)}
        if d is not None:
            meta["distance_km"] = float(d)
        Hr.add_edge(u, v, **meta)
        added.append((u, v, meta))

    return Hr, added
import networkx as nx
import numpy as np
import random
from geopy.distance import great_circle
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import splu
from typing import Any, List, Tuple, Dict, Optional


# ----------------------------
# 1) Build (grounded) Laplacian factorization
# ----------------------------
def _build_grounded_laplacian_lu(G: nx.Graph, node_list: List[Any]) -> Tuple[Dict[Any,int], int, Any, Any]:
    """
    Returns:
      idx: node->0..n-1
      g: ground index
      ground_node
      LU factorization of grounded Laplacian (n-1 x n-1)
    """
    n = len(node_list)
    idx = {u:i for i,u in enumerate(node_list)}
    g = n - 1                      # choose last node as ground
    ground_node = node_list[g]

    # adjacency (unweighted conductance=1). For weighted, use weight attr as conductance.
    rows, cols, data = [], [], []
    deg = np.zeros(n, dtype=float)

    for u, v in G.edges():
        iu, iv = idx[u], idx[v]
        w = 1.0
        if iu == iv:
            continue
        # off-diagonals -w
        rows += [iu, iv]
        cols += [iv, iu]
        data += [-w, -w]
        # diagonals accumulate degree
        deg[iu] += w
        deg[iv] += w

    # add diagonals
    rows += list(range(n))
    cols += list(range(n))
    data += deg.tolist()

    L = csr_matrix((data, (rows, cols)), shape=(n, n))

    # grounded Laplacian: remove row/col g -> SPD if connected
    mask = np.ones(n, dtype=bool)
    mask[g] = False
    Lg = L[mask][:, mask].tocsc()

    lu = splu(Lg)  # sparse LU; works well for n~few thousands
    return idx, g, ground_node, lu


def effective_resistance(lu, idx: Dict[Any,int], g: int, u: Any, v: Any) -> float:
    """
    Exact effective resistance for unweighted graph using one Laplacian solve:
      Solve L x = e_u - e_v with x_ground=0 (grounding removes singularity).
      R_eff(u,v) = (e_u - e_v)^T x = x[u] - x[v]
    """
    n = len(idx)
    iu, iv = idx[u], idx[v]

    # build b (n-1)
    b = np.zeros(n-1, dtype=float)

    def red(i):
        # map full index -> reduced index (skip ground g)
        return i if i < g else i - 1

    if iu != g:
        b[red(iu)] += 1.0
    if iv != g:
        b[red(iv)] -= 1.0

    x = lu.solve(b)  # reduced solution

    def x_full(i):
        if i == g:
            return 0.0
        return x[red(i)]

    return float(x_full(iu) - x_full(iv))


# ----------------------------
# 2) Candidate generation (random sampling + distance filter)
# ----------------------------
def geo_dist_km(G: nx.Graph, u: Any, v: Any) -> Optional[float]:
    nu, nv = G.nodes[u], G.nodes[v]
    if ("lat" in nu and "lon" in nu and "lat" in nv and "lon" in nv):
        try:
            return float(great_circle((nu["lat"], nu["lon"]), (nv["lat"], nv["lon"])).kilometers)
        except Exception:
            return None
    return None


def sample_candidate_edges(
    G: nx.Graph,
    max_candidates: int = 20000,
    max_distance_km: Optional[float] = 3000.0,
    seed: int = 0
) -> List[Tuple[Any, Any, Optional[float]]]:
    """
    Randomly sample non-edges as candidates, filtered by distance if coords exist.
    """
    rng = random.Random(seed)
    nodes = list(G.nodes())
    n = len(nodes)
    existing = set((u, v) if u <= v else (v, u) for u, v in G.edges())

    candidates = []
    tries = 0
    max_tries = max_candidates * 50  # avoid infinite loop

    while len(candidates) < max_candidates and tries < max_tries:
        tries += 1
        u = nodes[rng.randrange(n)]
        v = nodes[rng.randrange(n)]
        if u == v:
            continue
        a, b = (u, v) if u <= v else (v, u)
        if (a, b) in existing:
            continue

        d = geo_dist_km(G, u, v)
        if max_distance_km is not None and d is not None and d > max_distance_km:
            continue

        candidates.append((u, v, d))
        existing.add((a, b))  # prevent duplicates in candidate list too

    return candidates


# ----------------------------
# 3) Select k edges by highest effective resistance (TER-inspired heuristic)
# ----------------------------
def add_edges_by_effective_resistance(
    G: nx.Graph,
    k: int = 200,
    max_candidates: int = 20000,
    max_distance_km: Optional[float] = 3000.0,
    seed: int = 0
) -> Tuple[nx.Graph, List[Tuple[Any, Any, Dict[str, Any]]]]:
    """
    Build candidate edges, compute R_eff(u,v), add top-k.
    """
    if G.number_of_nodes() < 2:
        return G.copy(), []

    # work on LCC only (recommended)
    lcc = max(nx.connected_components(G), key=len)
    H = G.subgraph(lcc).copy()

    node_list = list(H.nodes())
    idx, g, ground_node, lu = _build_grounded_laplacian_lu(H, node_list)

    cand = sample_candidate_edges(H, max_candidates=max_candidates,
                                  max_distance_km=max_distance_km, seed=seed)

    scored = []
    for u, v, d in cand:
        reff = effective_resistance(lu, idx, g, u, v)
        scored.append((reff, u, v, d))

    scored.sort(reverse=True, key=lambda x: x[0])

    Hr = H.copy()
    added = []
    for reff, u, v, d in scored[:k]:
        meta = {"defense": "TER_like", "Reff": float(reff)}
        if d is not None:
            meta["distance_km"] = float(d)
        Hr.add_edge(u, v, **meta)
        added.append((u, v, meta))

    return Hr, added

G_def, added_edges = add_edges_by_effective_resistance(
    G,
    k=500,
    max_candidates=30000,
    max_distance_km=3000.0,
    seed=123
)
print("Added:", len(added_edges))

rand_LCC_def_ter, rand_DIA_def_ter, rand_APL_def_ter = random_removal_curves(G_def, FRACS, trials=TRIALS, seed=123)

k_approx_value_def = min(200, G_def.number_of_nodes() // 2)

targ_LCC_def_ter_degree, targ_DIA_def_ter_degree, targ_APL_def_ter_degree = targeted_degree_curves(G_def, FRACS, method="degree", mode=ATTACK_MODE)
targ_LCC_def_ter_pagerank, targ_DIA_def_ter_pagerank, targ_APL_def_ter_pagerank = targeted_degree_curves(G_def, FRACS, method="pagerank", mode=ATTACK_MODE)
targ_LCC_def_ter_betweenness, targ_DIA_def_ter_betweenness, targ_APL_def_ter_betweenness = targeted_degree_curves(G_def, FRACS, method="betweenness", mode=ATTACK_MODE, k_approx=k_approx_value_def)

R_rand_def_ter = R_index(FRACS, rand_LCC_def_ter)
R_targ_def_ter_degree = R_index(FRACS, targ_LCC_def_ter_degree)
R_targ_def_ter_pagerank = R_index(FRACS, targ_LCC_def_ter_pagerank)
R_targ_def_ter_betweenness = R_index(FRACS, targ_LCC_def_ter_betweenness)

print(f"R-index(LCC) — Random Ter: {R_rand_def_ter:.3f}")
print(f"R-index(LCC) — Targeted (Degree) Ter: {R_targ_def_ter_degree:.3f}")
print(f"R-index(LCC) — Targeted (PageRank) Ter: {R_targ_def_ter_pagerank:.3f}")
print(f"R-index(LCC) — Targeted (Betweenness) Ter: {R_targ_def_ter_betweenness:.3f}")

"""### Plot"""

# Plot LCC after defense
plot_robustness_curves(
    "OpenFlights Robustness — LCC (TER Defense)",
    "LCC size / N0",
    rand_LCC_def_ter, targ_LCC_def_ter_degree, targ_LCC_def_ter_pagerank, targ_LCC_def_ter_betweenness,
    "robustness_lcc_def_ter.png"
)

# Plot Diameter after defense
plot_robustness_curves(
    "OpenFlights Robustness — Diameter (approx.) (TER Defense)",
    "Approx. Diameter of LCC (hops)",
    rand_DIA_def_ter, targ_DIA_def_ter_degree, targ_DIA_def_ter_pagerank, targ_DIA_def_ter_betweenness,
    "robustness_diameter_def_ter.png"
)

# Plot Average Path Length after defense
plot_robustness_curves(
    "OpenFlights Robustness — Average Path Length (approx.) (TER Defense)",
    "Approx. Average Path Length",
    rand_APL_def_ter, targ_APL_def_ter_degree, targ_APL_def_ter_pagerank, targ_APL_def_ter_betweenness,
    "robustness_Average_Path_Length_def_ter.png"
)

"""## Schneider swap edge"""

import networkx as nx
import numpy as np
import random
from typing import Any, Dict, Tuple, List, Optional


# ---------- DSU ----------
class DSU:
    __slots__ = ("p", "sz")
    def __init__(self, n: int):
        self.p = list(range(n))
        self.sz = [1] * n

    def find(self, a: int) -> int:
        p = self.p
        while p[a] != a:
            p[a] = p[p[a]]
            a = p[a]
        return a

    def union(self, a: int, b: int) -> int:
        pa, pb = self.find(a), self.find(b)
        if pa == pb:
            return self.sz[pa]
        if self.sz[pa] < self.sz[pb]:
            pa, pb = pb, pa
        self.p[pb] = pa
        self.sz[pa] += self.sz[pb]
        return self.sz[pa]


def R_index(fracs: np.ndarray, curve: np.ndarray) -> float:
    return float(np.trapz(curve, fracs))


def _static_order_by_degree(G: nx.Graph) -> List[Any]:
    deg = dict(G.degree())
    return [n for n, _ in sorted(deg.items(), key=lambda x: (-x[1], str(x[0])))]


def lcc_curve_static_dsu(G: nx.Graph, fracs: np.ndarray, order: List[Any]) -> np.ndarray:
    """
    Compute S(f)=|LCC|/N0 for static-order removal using offline add-back DSU.
    order = nodes removed from first to last (static).
    """
    node_list = list(G.nodes())
    idx = {u: i for i, u in enumerate(node_list)}
    n = len(node_list)
    if n == 0:
        return np.zeros_like(fracs, dtype=float)

    # active nodes in add-back process
    active = [False] * n
    dsu = DSU(n)
    max_cc = 0

    # lcc_size_after_k_removed[k] for k=0..n
    # k removed == n - t active, where t added back.
    lcc_after_k = np.zeros(n + 1, dtype=int)
    lcc_after_k[n] = 0

    rev = list(reversed(order))  # add back in reverse removal order
    for t, u in enumerate(rev, start=1):
        iu = idx[u]
        active[iu] = True
        # union with active neighbors
        for v in G.adj[u]:
            iv = idx[v]
            if active[iv]:
                max_cc = max(max_cc, dsu.union(iu, iv))
        max_cc = max(max_cc, 1)
        k_removed = n - t
        lcc_after_k[k_removed] = max_cc

    ks = np.clip(np.rint(fracs * n).astype(int), 0, n)
    return lcc_after_k[ks].astype(float) / float(n)


def robustness_R_static_fast(G: nx.Graph, fracs: np.ndarray, order: List[Any]) -> float:
    curve = lcc_curve_static_dsu(G, fracs, order)
    return R_index(fracs, curve)


# ---------- Schneider optimizer (fast) ----------
def optimize_schneider_fast(
    G: nx.Graph,
    fracs: Optional[np.ndarray] = None,
    max_trials: int = 20000,
    patience: int = 5000,
    min_delta_R: float = 1e-6,
    seed: int = 0,

    # prefilter strength: larger -> fewer R evaluations
    prefilter: bool = True
) -> Tuple[nx.Graph, Dict[str, Any]]:

    if fracs is None:
        fracs = np.linspace(0, 0.3, 21)

    rng = random.Random(seed)
    Gr = G.copy()

    # IMPORTANT: static order fixed because degree-preserving swaps keep degree
    order = _static_order_by_degree(Gr)
    deg = dict(Gr.degree())

    R_best = robustness_R_static_fast(Gr, fracs, order)
    edges = list(Gr.edges())

    accepted = 0
    no_improve = 0
    r_evals = 1  # already computed R_best

    def score_degree_mixing(e1, e2, ne1, ne2) -> int:
        """Lower is more onion-like: connect similar degrees."""
        (a, b), (c, d) = e1, e2
        (x1, y1), (x2, y2) = ne1, ne2
        old = abs(deg[a]-deg[b]) + abs(deg[c]-deg[d])
        new = abs(deg[x1]-deg[y1]) + abs(deg[x2]-deg[y2])
        return new - old  # negative means improved (more assortative)

    for t in range(1, max_trials + 1):
        if no_improve >= patience or Gr.number_of_edges() < 2:
            break

        e1, e2 = rng.sample(edges, 2)
        if len(set(e1 + e2)) < 4:
            no_improve += 1
            continue

        # try both swap variants, but prefilter before expensive R
        best_local = None  # (R_new, ne1, ne2)
        for ne1, ne2 in [((e1[0], e2[0]), (e1[1], e2[1])),
                         ((e1[0], e2[1]), (e1[1], e2[0]))]:

            # validity checks
            if ne1[0] == ne1[1] or ne2[0] == ne2[1]:
                continue
            if Gr.has_edge(*ne1) or Gr.has_edge(*ne2):
                continue
            if set(ne1) == set(ne2):
                continue

            if prefilter:
                # if it doesn't improve degree-mixing, skip without R eval
                if score_degree_mixing(e1, e2, ne1, ne2) >= 0:
                    continue

            # apply temporarily
            Gr.remove_edge(*e1); Gr.remove_edge(*e2)
            Gr.add_edge(*ne1); Gr.add_edge(*ne2)

            R_new = robustness_R_static_fast(Gr, fracs, order)
            r_evals += 1

            # revert
            Gr.remove_edge(*ne1); Gr.remove_edge(*ne2)
            Gr.add_edge(*e1); Gr.add_edge(*e2)

            if best_local is None or R_new > best_local[0]:
                best_local = (R_new, ne1, ne2)

        if best_local is not None and best_local[0] > R_best + min_delta_R:
            R_new, ne1, ne2 = best_local
            Gr.remove_edge(*e1); Gr.remove_edge(*e2)
            Gr.add_edge(*ne1); Gr.add_edge(*ne2)

            R_best = R_new
            accepted += 1
            no_improve = 0
            edges = list(Gr.edges())
        else:
            no_improve += 1

    info = {
        "R_best_static": float(R_best),
        "accepted_swaps": accepted,
        "trials_done": t,
        "R_evaluations": r_evals,
        "stopped_by_patience": (no_improve >= patience),
        "fracs_points": int(len(fracs)),
        "prefilter": prefilter,
    }
    return Gr, info

# 1) Tối ưu nhanh theo static (để chạy được)
G_opt, info = optimize_schneider_fast(
    G,
    fracs=np.linspace(0, 0.3, 21),
    max_trials=50000,
    patience=8000,
    seed=123,
    prefilter=True
)
print(info)
rand_LCC_def_sch, rand_DIA_def_sch, rand_APL_def_sch = random_removal_curves(G_opt, FRACS, trials=TRIALS, seed=123)

k_approx_value_sch = min(200, G_opt.number_of_nodes() // 2)

targ_LCC_def_sch_degree, targ_DIA_def_sch_degree, targ_APL_def_sch_degree = targeted_degree_curves(G_opt, FRACS, method="degree", mode=ATTACK_MODE)
targ_LCC_def_sch_pagerank, targ_DIA_def_sch_pagerank, targ_APL_def_sch_pagerank = targeted_degree_curves(G_opt, FRACS, method="pagerank", mode=ATTACK_MODE)
targ_LCC_def_sch_betweenness, targ_DIA_def_sch_betweenness, targ_APL_def_sch_betweenness = targeted_degree_curves(G_opt, FRACS, method="betweenness", mode=ATTACK_MODE, k_approx=k_approx_value_sch)

R_rand_def_sch = R_index(FRACS, rand_LCC_def_sch)
R_targ_def_sch_degree = R_index(FRACS, targ_LCC_def_sch_degree)
R_targ_def_sch_pagerank = R_index(FRACS, targ_LCC_def_sch_pagerank)
R_targ_def_sch_betweenness = R_index(FRACS, targ_LCC_def_sch_betweenness)

print(f"R-index(LCC) — Random onion: {R_rand_def_sch:.3f}")
print(f"R-index(LCC) — Targeted (Degree) onion: {R_targ_def_sch_degree:.3f}")
print(f"R-index(LCC) — Targeted (PageRank) onion: {R_targ_def_sch_pagerank:.3f}")
print(f"R-index(LCC) — Targeted (Betweenness) onion: {R_targ_def_sch_betweenness:.3f}")

# Plot LCC after Schneider defense
plot_robustness_curves(
    "OpenFlights Robustness — LCC (Schneider Defense)",
    "LCC size / N0",
    rand_LCC_def_sch, targ_LCC_def_sch_degree, targ_LCC_def_sch_pagerank, targ_LCC_def_sch_betweenness,
    "robustness_lcc_def_sch.png"
)

# Plot Diameter after Schneider defense
plot_robustness_curves(
    "OpenFlights Robustness — Diameter (approx.) (Schneider Defense)",
    "Approx. Diameter of LCC (hops)",
    rand_DIA_def_sch, targ_DIA_def_sch_degree, targ_DIA_def_sch_pagerank, targ_DIA_def_sch_betweenness,
    "robustness_diameter_def_sch.png"
)

# Plot Average Path Length after Schneider defense
plot_robustness_curves(
    "OpenFlights Robustness — Average Path Length (approx.) (Schneider Defense)",
    "Approx. Average Path Length",
    rand_APL_def_sch, targ_APL_def_sch_degree, targ_APL_def_sch_pagerank, targ_APL_def_sch_betweenness,
    "robustness_Average_Path_Length_def_sch.png"
)

"""# Other"""

from networkx.algorithms.community import greedy_modularity_communities

# Top-15 hubs
deg = pd.Series(dict(G.degree())).sort_values(ascending=False)
top15 = deg.head(15).reset_index(); top15.columns=["airport_id","degree"]
top15 = top15.merge(ap[["airport_id","name","city","country","iata","icao"]], on="airport_id", how="left")
top15.to_csv(OUT_DIR/"top15_hubs.csv", index=False)
top15.head(10)

# 5 largest communities & their top-degree hubs
comms = list(greedy_modularity_communities(G))
deg_all = dict(G.degree())
rows = []
for cid, comm in enumerate(sorted(comms, key=lambda c: -len(c))[:5]):
    hub = max(comm, key=lambda u: deg_all[u])
    rec = ap.loc[ap["airport_id"]==hub, ["airport_id","name","city","country","iata","icao"]].iloc[0].to_dict()
    rec["community_id"] = cid
    rec["community_size"] = len(comm)
    rec["degree"] = int(deg_all[hub])
    rows.append(rec)
comm_df = pd.DataFrame(rows)
comm_df.to_csv(OUT_DIR/"top_community_hubs.csv", index=False)
comm_df

# Cross-community hub pairs without direct edges (redundancy candidates)
pairs = []
hubs = list(comm_df["airport_id"])
for i in range(len(hubs)):
    for j in range(i+1, len(hubs)):
        u, v = int(hubs[i]), int(hubs[j])
        connected = G.has_edge(u, v)
        ui = ap.loc[ap["airport_id"]==u, ["iata","city","country"]].iloc[0]
        vi = ap.loc[ap["airport_id"]==v, ["iata","city","country"]].iloc[0]
        pairs.append({"u_id":u,"u_iata":ui["iata"],"u_city":ui["city"],"u_country":ui["country"],
                      "v_id":v,"v_iata":vi["iata"],"v_city":vi["city"],"v_country":vi["country"],
                      "connected":bool(connected)})
cand = pd.DataFrame(pairs).query("connected == False")
cand.to_csv(OUT_DIR/"candidate_redundancy_pairs.csv", index=False)
cand

# per-country: nodes & edge incidents
nodes = pd.DataFrame({"airport_id": list(G.nodes())}).merge(ap[["airport_id","country"]], on="airport_id", how="left")
node_counts = nodes.groupby("country", dropna=False)["airport_id"].count().reset_index(name="num_airports")

edge_rows = []
for u, v in G.edges():
    cu = ap.loc[ap["airport_id"]==u, "country"].values[0]
    cv = ap.loc[ap["airport_id"]==v, "country"].values[0]
    edge_rows.append({"country": cu})
    edge_rows.append({"country": cv})
edge_counts = pd.DataFrame(edge_rows).groupby("country", dropna=False).size().reset_index(name="edge_incidents")

country_stats = node_counts.merge(edge_counts, on="country", how="outer").fillna(0)
country_stats = country_stats.sort_values(["edge_incidents","num_airports"], ascending=False)
country_stats.to_csv(OUT_DIR/"country_stats.csv", index=False)
country_stats.head(10)

# explode equipment tokens & map to planes
eq_list = []
for eq in r_clean["equipment"].tolist():
    if not eq: continue
    for token in str(eq).split():
        eq_list.append(token.strip().upper())
eq_df = pd.DataFrame({"equipment_token": eq_list})

if not eq_df.empty:
    pl_iata = pl.dropna(subset=["iata_code"])[["iata_code","name"]].rename(columns={"iata_code":"equipment_token","name":"plane_name_iata"})
    pl_icao = pl.dropna(subset=["icao_code"])[["icao_code","name"]].rename(columns={"icao_code":"equipment_token","name":"plane_name_icao"})

    eq_stats = eq_df.value_counts("equipment_token").reset_index(name="count")
    eq_stats = eq_stats.merge(pl_iata, on="equipment_token", how="left")
    eq_stats = eq_stats.merge(pl_icao, on="equipment_token", how="left")
    eq_stats["plane_name"] = eq_stats["plane_name_iata"].combine_first(eq_stats["plane_name_icao"])
    eq_stats["mapped"] = ~eq_stats["plane_name"].isna()
    coverage = eq_stats["mapped"].mean()
    eq_stats.to_csv(OUT_DIR/"equipment_stats.csv", index=False)
    with open(OUT_DIR/"equipment_stats_README.txt","w") as f:
        f.write(f"Equipment tokens mapped to Planes: {coverage:.2%} coverage (by unique tokens)\\n")
    eq_stats.head(15)
else:
    print("No equipment tokens found in routes.")

rt_air = rt.copy()
rt_air["airline_key"] = rt_air["airline_id"].astype("Int64").astype(str).where(rt_air["airline_id"].notna(), rt_air["airline_code"].astype(str))

src = rt_air.groupby("src_id")["airline_key"].nunique().reset_index(name="num_airlines_src")
dst = rt_air.groupby("dst_id")["airline_key"].nunique().reset_index(name="num_airlines_dst")
div = src.merge(dst, left_on="src_id", right_on="dst_id", how="outer")
div["airport_id"] = div["src_id"].fillna(div["dst_id"]).astype("Int64")
div["airline_diversity"] = div["num_airlines_src"].fillna(0) + div["num_airlines_dst"].fillna(0)

out = div.merge(ap[["airport_id","name","city","country","iata","icao"]], on="airport_id", how="left")
out = out[["airport_id","name","city","country","iata","icao","airline_diversity"]].sort_values("airline_diversity", ascending=False)
out.to_csv(OUT_DIR/"airport_airline_diversity.csv", index=False)
out.head(10)

import pandas as pd

# Giả sử bạn đã có Hr và added_edges từ hàm add_edges_by_effective_resistance
# Hr, added_edges = add_edges_by_effective_resistance(...)

# 1. Chuẩn bị dữ liệu cho bảng
table_data = []

for idx, (u, v, meta) in enumerate(added_edges, 1):
    # Lấy thông tin từ node attributes trong đồ thị Hr
    u_info = G_def.nodes[u]
    v_info = G_def.nodes[v]

    row = {
        "Rank": idx,
        "City A": u_info.get('city', 'Unknown'),
        "IATA A": u_info.get('iata', str(u)), # Dùng IATA cho gọn
        "City B": v_info.get('city', 'Unknown'),
        "IATA B": v_info.get('iata', str(v)),
        "Eff. Resistance": meta.get('Reff', 0),
        "Distance (km)": meta.get('distance_km', 0)
    }
    table_data.append(row)

# 2. Tạo DataFrame
df_added = pd.DataFrame(table_data)

# 3. In ra dạng bảng Markdown (đẹp và dễ đọc)
# floatfmt=".4f" để làm tròn số thực cho gọn
print(df_added.to_markdown(index=False, floatfmt=".4f"))

# Nếu muốn lưu ra file CSV để báo cáo:
# df_added.to_csv("added_edges_report.csv", index=False)

"""# Case Study

## SGN -> CFN
"""

import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
from geopy.distance import great_circle

# Giả sử bạn đã có các biến toàn cục: G, G_def, ap (dataframe sân bay)
# Nếu chưa có G_def, hãy đảm bảo bạn đã chạy hàm add_edges_by_effective_resistance trước đó

# ------------------------------------------------------------------------------
# 1. HÀM TÍNH KHOẢNG CÁCH (WEIGHTING)
# ------------------------------------------------------------------------------
def add_distance_weights(graph, airports_df):
    """
    Tính và thêm trọng số 'weight' (khoảng cách km) cho tất cả các cạnh trong đồ thị.
    Dùng cho cả G gốc và G_def.
    """
    print(f"Calculating edge distances for graph with {graph.number_of_edges()} edges...")

    # Tạo dictionary tra cứu toạ độ nhanh
    coords = airports_df.set_index("airport_id")[["lat", "lon"]].to_dict("index")

    count = 0
    for u, v, data in graph.edges(data=True):
        # Nếu cạnh đã có weight, bỏ qua để tiết kiệm thời gian
        if "weight" in data:
            continue

        if u in coords and v in coords:
            pos_u = (coords[u]["lat"], coords[u]["lon"])
            pos_v = (coords[v]["lat"], coords[v]["lon"])
            try:
                dist_km = great_circle(pos_u, pos_v).kilometers
                data["weight"] = dist_km
            except ValueError:
                data["weight"] = 99999.0
        else:
            data["weight"] = 99999.0

        count += 1

    print(f"-> Added weights to {count} edges.")
    return graph

# ------------------------------------------------------------------------------
# 2. HÀM PHÂN TÍCH LỘ TRÌNH ĐƠN (SINGLE ROUTE ANALYSIS)
# ------------------------------------------------------------------------------
def analyze_flight_route(G, src_iata, dst_iata, attack_nodes_iata=None):
    # Tìm Airport ID
    try:
        src_row = ap[ap["iata"] == src_iata]
        dst_row = ap[ap["iata"] == dst_iata]
        if src_row.empty or dst_row.empty:
            print(f"Error: Invalid IATA code {src_iata} or {dst_iata}")
            return

        src_id = src_row.iloc[0]["airport_id"]
        dst_id = dst_row.iloc[0]["airport_id"]
        src_name = src_row.iloc[0]["name"]
        dst_name = dst_row.iloc[0]["name"]
    except Exception as e:
        print(f"Error looking up airports: {e}")
        return

    print(f"\n{'='*60}")
    print(f"CASE STUDY: {src_iata} ({src_name}) -> {dst_iata} ({dst_name})")
    print(f"{'='*60}")

    # --- A. BEFORE ATTACK ---
    try:
        path = nx.shortest_path(G, source=src_id, target=dst_id, weight="weight")
        dist = nx.shortest_path_length(G, source=src_id, target=dst_id, weight="weight")

        print(f"\n[1] ORIGINAL ROUTE (Total: {dist:.2f} km)")
        route_str = [G.nodes[n].get("iata", str(n)) for n in path]
        print(f"   Route: {' -> '.join(route_str)}")
        print(f"   Hops: {len(path)-1}")

    except nx.NetworkXNoPath:
        print(f"No path found between {src_iata} and {dst_iata}")
        return

    # --- B. ATTACK SCENARIO ---
    target_ids = []
    if attack_nodes_iata:
        for code in attack_nodes_iata:
            row = ap[ap["iata"] == code]
            if not row.empty:
                target_ids.append(row.iloc[0]["airport_id"])

    if not target_ids:
        print("\n   (!) No attack targets specified.")
        return

    print(f"\n[2] ATTACK SIMULATION")
    print(f"   Removing nodes: {attack_nodes_iata}")

    G_attack = G.copy()
    G_attack.remove_nodes_from(target_ids)

    # --- C. AFTER ATTACK ---
    try:
        new_path = nx.shortest_path(G_attack, source=src_id, target=dst_id, weight="weight")
        new_dist = nx.shortest_path_length(G_attack, source=src_id, target=dst_id, weight="weight")

        print(f"\n[3] REROUTED PATH (Total: {new_dist:.2f} km)")
        new_route_str = [G.nodes[n].get("iata", str(n)) for n in new_path]
        print(f"   New Route: {' -> '.join(new_route_str)}")
        print(f"   Impact: +{new_dist - dist:.2f} km")

    except nx.NetworkXNoPath:
        print(f"\n[3] SYSTEM FAILURE: Network disconnected!")

# ------------------------------------------------------------------------------
# 3. HÀM MÔ PHỎNG TẤN CÔNG THÍCH NGHI & SO SÁNH (ADAPTIVE ATTACK & PLOT)
# ------------------------------------------------------------------------------
def adaptive_route_attack_simulation(G, G_def, src_iata, dst_iata):
    try:
        src_id = ap[ap["iata"] == src_iata].iloc[0]["airport_id"]
        dst_id = ap[ap["iata"] == dst_iata].iloc[0]["airport_id"]
    except:
        print("Error: Mã sân bay không hợp lệ.")
        return

    print(f"\n{'='*70}")
    print(f"ADAPTIVE ATTACK SIMULATION: {src_iata} -> {dst_iata}")
    print(f"{'='*70}")

    # Tìm đường đi ban đầu để xác định node cần tấn công
    try:
        path = nx.shortest_path(G, source=src_id, target=dst_id, weight="weight")
        transit_ids = path[1:-1]
        transit_codes = [ap[ap['airport_id']==nid].iloc[0]['iata'] for nid in transit_ids]

        print(f"Current Optimal Route: {' -> '.join([ap[ap['airport_id']==n].iloc[0]['iata'] for n in path])}")
        print(f"Critical Transit Nodes: {transit_codes}")

        if not transit_codes:
            print("(!) Đường bay thẳng. Không có transit để tấn công.")
            return
    except nx.NetworkXNoPath:
        print("Không có đường bay ban đầu.")
        return

    # Chuẩn bị dữ liệu vẽ biểu đồ
    results_orig = []
    results_def = []
    labels = []

    # Kịch bản 0: Baseline (Không tấn công)
    d_base_orig = nx.shortest_path_length(G, src_id, dst_id, weight="weight")
    try:
        d_base_def = nx.shortest_path_length(G_def, src_id, dst_id, weight="weight")
    except: d_base_def = float('inf')

    results_orig.append(d_base_orig)
    results_def.append(d_base_def)
    labels.append("None")

    print("\n--- ATTACK RESULTS ---")
    print(f"{'Target':<10} | {'Original (km)':<15} | {'Defended (km)':<15} | {'Delta'}")
    print("-" * 60)

    # Kịch bản 1..N: Tấn công từng node
    for target_code, target_id in zip(transit_codes, transit_ids):
        # Tấn công G
        G_temp = G.copy()
        G_temp.remove_node(target_id)
        try: d_orig = nx.shortest_path_length(G_temp, src_id, dst_id, weight="weight")
        except: d_orig = float('inf')

        # Tấn công G_def
        G_def_temp = G_def.copy()
        G_def_temp.remove_node(target_id)
        try: d_def = nx.shortest_path_length(G_def_temp, src_id, dst_id, weight="weight")
        except: d_def = float('inf')

        results_orig.append(d_orig)
        results_def.append(d_def)
        labels.append(f"-{target_code}")

        # In kết quả
        s_orig = f"{d_orig:.1f}" if d_orig != float('inf') else "DEAD"
        s_def = f"{d_def:.1f}" if d_def != float('inf') else "DEAD"
        print(f"{target_code:<10} | {s_orig:<15} | {s_def:<15} | {d_def - d_orig if d_orig!=float('inf') and d_def!=float('inf') else '---'}")

    # Kịch bản Cuối: Combo Attack (DUB + GLA) - Cắt cửa ngõ Donegal
    combo_targets = ['DUB', 'GLA'] # Bạn có thể thêm các hub khác vào đây
    combo_ids = [ap[ap['iata']==c].iloc[0]['airport_id'] for c in combo_targets if len(ap[ap['iata']==c]) > 0]

    if combo_ids:
        # Tấn công G
        G_temp = G.copy()
        G_temp.remove_nodes_from(combo_ids)
        try: d_orig = nx.shortest_path_length(G_temp, src_id, dst_id, weight="weight")
        except: d_orig = float('inf')

        # Tấn công G_def
        G_def_temp = G_def.copy()
        G_def_temp.remove_nodes_from(combo_ids)
        try: d_def = nx.shortest_path_length(G_def_temp, src_id, dst_id, weight="weight")
        except: d_def = float('inf')

        results_orig.append(d_orig)
        results_def.append(d_def)
        labels.append("Combo")
        print(f"{'Combo':<10} | {d_orig if d_orig!=float('inf') else 'DEAD':<15} | {d_def if d_def!=float('inf') else 'DEAD':<15} | ---")

    # Vẽ biểu đồ
    max_val = max([v for v in results_orig + results_def if v != float('inf')] + [10000]) * 1.2
    plot_orig = [v if v != float('inf') else max_val for v in results_orig]
    plot_def = [v if v != float('inf') else max_val for v in results_def]

    x = range(len(labels))
    width = 0.35
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar([i - width/2 for i in x], plot_orig, width, label='Original', color='#ff9999')
    ax.bar([i + width/2 for i in x], plot_def, width, label='Defended', color='#66b3ff')

    # Đánh dấu X
    for i, v in enumerate(results_orig):
        if v == float('inf'): ax.text(i - width/2, max_val*0.95, 'X', ha='center', color='red', fontweight='bold', fontsize=14)
    for i, v in enumerate(results_def):
        if v == float('inf'): ax.text(i + width/2, max_val*0.95, 'X', ha='center', color='red', fontweight='bold', fontsize=14)

    ax.set_ylabel('Path Length (km)')
    ax.set_title(f'Attack Simulation: {src_iata} -> {dst_iata}')
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()
    plt.tight_layout()
    plt.show()

# ------------------------------------------------------------------------------
# 4. CHẠY THỰC TẾ
# ------------------------------------------------------------------------------

# B1. Tính weight (quan trọng!)
add_distance_weights(G, ap)
add_distance_weights(G_def, ap)

# B2. Chạy so sánh adaptive
adaptive_route_attack_simulation(G, G_def, "SGN", "CFN")